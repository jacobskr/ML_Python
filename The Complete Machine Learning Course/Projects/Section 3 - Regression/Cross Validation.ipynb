{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation (CV)\n",
    "\n",
    " - Hold out Cross Validation\n",
    " - k-fold Cross Validation\n",
    " \n",
    "A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV.\n",
    "\n",
    "\n",
    "In the basic approach, called k-fold CV, the training set is split into k smaller sets. The following procedure is followed for each of the k \"folds\":\n",
    " - A model is trained using k-1 of the folds as the training data;\n",
    " - the resulting model is validated on the remaining part of the data.\n",
    " \n",
    "The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop.\n",
    "\n",
    "### Holdout Method\n",
    "\n",
    " - Split inital dataest into separate training and test datasest\n",
    " - trainign dataset - model training\n",
    " - Test dataset = estimate its generalized performance\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "A variation is to split the training set into two := training set and validation set\n",
    "\n",
    "Training set :- For fitting different models\n",
    "\n",
    "Validation set :- For tuning and comparing different parameter settings to further imporve the performance for making predictions on unseen data and final model selection.\n",
    "\n",
    "This process is called model selection. We want to select the optimal values of tuning parameters.\n",
    "\n",
    "### k-fold\n",
    "\n",
    " - Randomly split the training dataset into k fold without replacememt.\n",
    " - k - 1 folds are used for the model training.\n",
    " - The one fold is used for performance evaluation.\n",
    " \n",
    "The procedure is repeated k times.\n",
    "\n",
    "Final outbomes:- k models and performance estimates.\n",
    " - calculate the average peprformance of the models based on the differen independent folds to obtain a performance estimate that is less sensitive to the sub-partitioning of the training data compared to the holdout method.\n",
    " - k-fold CV is used for model tuning. Finding the optimal hyperparameter values that yields a satisfying generalization performance.\n",
    " - Once we have found satisfactory hyperparameter values, we can retrain the model on the complete training set and obtain a final performance estimate using the independent test set. The rationale behind fitting a model to the whole training dataset after k-fold CV is that prividing more training samples to a learning algorithm usually results in a more accurate and robust model.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    " - Common k is 10.\n",
    " - For relatively small training sets, increase the number of folds.\n",
    " \n",
    "### Stratified k-fold cross-validation\n",
    " \n",
    "  - variation of k-fold\n",
    "  - Can yield better bias and variance estimates, especially in cases of unequal class proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration\n",
    "### Cross-validation: evaluating estimator performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((506, 13), (506,))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "boston.data.shape, boston.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now quickly sample a training set while holding out 40% of the data for testing our regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6672554157940424"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "boston.data, boston.target, test_size=0.4, random_state=0)\n",
    "\n",
    "X_train.shape, y_train.shape\n",
    "\n",
    "X_test.shape, y_test.shape\n",
    "\n",
    "regression = svm.SVR(kernel='linear', C=1).fit(X_train, y_train)\n",
    "regression.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cross-validated metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.77328953, 0.72833447, 0.53795481, 0.15209389, 0.07729196])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import  cross_val_score\n",
    "regression = svm.SVR(kernel='linear', C=1)\n",
    "scores = cross_val_score(regression, boston.data, boston.target, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean score and the 95% CI of the score estimate are hence given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.45 (+/- 0.58)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the score computer at each CV iteration is the score method of the estimator. It is possible to change this by using the scoring parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -7.82949025, -24.73154773, -37.00390719, -74.37141515,\n",
       "       -24.53325372])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "scores = cross_val_score(regression, boston.data, boston.target, cv=5, scoring='neg_mean_squared_error')\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold\n",
    "\n",
    "KFol divides all the sameples in k groups of samples, called folds, of equal sizes (if possible). The prediction function is learned using k-1 folds, and the fold left out is used for test.\n",
    "\n",
    "Example of 2-fold CV on a dataset with 4 samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3] [0 1]\n",
      "[0 1] [2 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = [\"a\", \"b\", \"c\", \"d\"]\n",
    "kf = KFold(n_splits=2)\n",
    "for train, test in kf.split(X):\n",
    "    print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified k-fold\n",
    "\n",
    "StraifiedKFold is a variation of k-fold which returns stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set.\n",
    "\n",
    "Example of stratified 3-fold CV on a dataset with 10 sameples from two slightly unbalanced classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 6 7 8 9] [0 1 4 5]\n",
      "[0 1 3 4 5 8 9] [2 6 7]\n",
      "[0 1 2 4 5 6 7] [3 8 9]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import  StratifiedKFold\n",
    "\n",
    "X= np.ones(10)\n",
    "y = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "for train, test in skf.split(X, y):\n",
    "    print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.391\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe_svm = make_pipeline(StandardScaler(),\n",
    "                        PCA(n_components=2),\n",
    "                        svm.SVR(kernel='linear', C=1))\n",
    "pipe_svm.fit(X_train, y_train)\n",
    "y_pred = pipe_svm.predict(X_test)\n",
    "print('Test Accuracy: %.3f' % pipe_svm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy scores: [0.63971176 0.43579197 0.46977821 0.25027246 0.5124364  0.26221374\n",
      " 0.30877195 0.54528563 0.37810066 0.47313549]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(estimator=pipe_svm,\n",
    "                        X=X_train, y=y_train,\n",
    "                        cv=10, n_jobs=1)\n",
    "print('CV accuracy scores: %s' % scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy: 0.428 =/- 0.121\n"
     ]
    }
   ],
   "source": [
    "print('CV accuracy: %.3f =/- %.3f' % (np.mean(scores),\n",
    "                                     np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
