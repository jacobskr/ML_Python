{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting (Hypothesis Boosting)\n",
    " - Combine several weak learners into a strong leaner.\n",
    " - Train predictors sequentially.\n",
    " \n",
    "## AdaBoost / Adaptive Boosting\n",
    "\n",
    "As above for Boosting:\n",
    " - Similar to human learning, the algorithm learns from past mistakes by focusing more on difficult problems it did not get right in prior learning.\n",
    " - It pays more attention to training instances that previously underfit.\n",
    " \n",
    " \n",
    " - Fit a sequence of weak learners (models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data.\n",
    " - The precistions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction.\n",
    " - The data modifications at each boosting iteration consist of applying weights $w_1, w_2, ... , w_n$ for each of the training samples.\n",
    " - Initially, those weights are all set to $w_i=1/N$, so that the first step simply trains a weak learner on the original data.\n",
    " - For each successive iteration, the sample weights are indiviually modified and the learning algorithm is reapplied to the reweighted data.\n",
    " - At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly.\n",
    " - As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "df.dropna(inplace=True)\n",
    "X = df[['pclass', 'sex', 'age']]\n",
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "X['sex'] = lb.fit_transform(X['sex'])\n",
    "y = df['survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(clf, X_train, y_train, X_test, y_test, train=True):\n",
    "    '''\n",
    "    print the accuracy score, classification report, and confusion matrix of classifier\n",
    "    '''\n",
    "    if train:\n",
    "        '''\n",
    "        Training Performance\n",
    "        '''\n",
    "        print(\"Train Result:\\n\")\n",
    "        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_train, clf.predict(X_train))))\n",
    "        print(\"Classification Report:\\n {} \\n\".format(classification_report(y_train, clf.predict(X_train))))\n",
    "        print(\"Confusion Matrix:  \\n {} \\n\".format(confusion_matrix(y_train, clf.predict(X_train))))\n",
    "        \n",
    "        res = cross_val_score(clf, X_train, y_train, cv=10, scoring='accuracy')\n",
    "        print(\"Average Accuracy: \\t {0:.4f}\".format(np.mean(res)))\n",
    "        print(\"Accuracy SD \\t\\t {0:.4f}\".format(np.std(res)))\n",
    "        \n",
    "    elif train==False:\n",
    "        '''\n",
    "        test performance\n",
    "        '''\n",
    "        print(\"Test Result:\\n\")\n",
    "        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_test, clf.predict(X_test))))\n",
    "        print(\"Classification Report:\\n {} \\n\".format(classification_report(y_test, clf.predict(X_test))))\n",
    "        print(\"Confusion Matrix:  \\n {} \\n\".format(confusion_matrix(y_test, clf.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_clf = AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "\n",
      "accuracy score: 0.8819\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.86      0.84        44\n",
      "           1       0.93      0.89      0.91        83\n",
      "\n",
      "   micro avg       0.88      0.88      0.88       127\n",
      "   macro avg       0.87      0.88      0.87       127\n",
      "weighted avg       0.88      0.88      0.88       127\n",
      " \n",
      "\n",
      "Confusion Matrix:  \n",
      " [[38  6]\n",
      " [ 9 74]] \n",
      "\n",
      "Average Accuracy: \t 0.7174\n",
      "Accuracy SD \t\t 0.1236\n"
     ]
    }
   ],
   "source": [
    "print_score(ada_clf, X_train, y_train, X_test, y_test, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "\n",
      "accuracy score: 0.7273\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.60      0.55        15\n",
      "           1       0.84      0.78      0.81        40\n",
      "\n",
      "   micro avg       0.73      0.73      0.73        55\n",
      "   macro avg       0.67      0.69      0.68        55\n",
      "weighted avg       0.75      0.73      0.73        55\n",
      " \n",
      "\n",
      "Confusion Matrix:  \n",
      " [[ 9  6]\n",
      " [ 9 31]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_score(ada_clf, X_train, y_train, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_clf = AdaBoostClassifier(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "\n",
      "accuracy score: 0.9528\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        44\n",
      "           1       0.96      0.96      0.96        83\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       127\n",
      "   macro avg       0.95      0.95      0.95       127\n",
      "weighted avg       0.95      0.95      0.95       127\n",
      " \n",
      "\n",
      "Confusion Matrix:  \n",
      " [[41  3]\n",
      " [ 3 80]] \n",
      "\n",
      "Average Accuracy: \t 0.7650\n",
      "Accuracy SD \t\t 0.1295\n"
     ]
    }
   ],
   "source": [
    "print_score(ada_clf, X_train, y_train, X_test, y_test, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "\n",
      "accuracy score: 0.7273\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.60      0.55        15\n",
      "           1       0.84      0.78      0.81        40\n",
      "\n",
      "   micro avg       0.73      0.73      0.73        55\n",
      "   macro avg       0.67      0.69      0.68        55\n",
      "weighted avg       0.75      0.73      0.73        55\n",
      " \n",
      "\n",
      "Confusion Matrix:  \n",
      " [[ 9  6]\n",
      " [ 9 31]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_score(ada_clf, X_train, y_train, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Try with grid search and increasing n_estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {'max_depth': [3, 4, 5, None],\n",
    "               'min_samples_split': range(2, 10),\n",
    "               'min_samples_leaf': range(2, 10),\n",
    "               'criterion': ['gini', 'entropy'],\n",
    "               'n_estimators': [500]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(rf_clf, params_grid,\n",
    "                          n_jobs=-1, cv=5,\n",
    "                          verbose=1, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 512 candidates, totalling 2560 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   22.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   60.0s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed: 10.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2560 out of 2560 | elapsed: 11.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'max_depth': [3, 4, 5, None], 'min_samples_split': range(2, 10), 'min_samples_leaf': range(2, 10), 'criterion': ['gini', 'entropy'], 'n_estimators': [500]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 2,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 500,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': 42,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(max_depth=None, min_samples_leaf=2, \n",
    "                                                    min_samples_split=2, criterion='gini',\n",
    "                                                    n_estimators=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_clf = AdaBoostClassifier(rf_clf, n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=2, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          learning_rate=1.0, n_estimators=100, random_state=None)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "\n",
      "accuracy score: 0.9449\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92        44\n",
      "           1       0.95      0.96      0.96        83\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       127\n",
      "   macro avg       0.94      0.94      0.94       127\n",
      "weighted avg       0.94      0.94      0.94       127\n",
      " \n",
      "\n",
      "Confusion Matrix:  \n",
      " [[40  4]\n",
      " [ 3 80]] \n",
      "\n",
      "Average Accuracy: \t 0.7900\n",
      "Accuracy SD \t\t 0.1213\n"
     ]
    }
   ],
   "source": [
    "print_score(ada_clf, X_train, y_train, X_test, y_test, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "\n",
      "accuracy score: 0.7273\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.60      0.55        15\n",
      "           1       0.84      0.78      0.81        40\n",
      "\n",
      "   micro avg       0.73      0.73      0.73        55\n",
      "   macro avg       0.67      0.69      0.68        55\n",
      "weighted avg       0.75      0.73      0.73        55\n",
      " \n",
      "\n",
      "Confusion Matrix:  \n",
      " [[ 9  6]\n",
      " [ 9 31]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_score(ada_clf, X_train, y_train, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting / Gradient Boosting Machine (GBM)\n",
    "Works for both regression and classification\n",
    "\n",
    " - Sequentially adding predictors\n",
    " - Each one correcting it's predecessor\n",
    " - Fit new predictor to the residual errors\n",
    " \n",
    " **Step 1.**\n",
    " $$Y=F(x)+\\epsilon$$\n",
    " **Step 2.**\n",
    " $$\\epsilon=G(x)+\\epsilon_2$$\n",
    " Subsituting (2) into (1), we get:\n",
    " $$Y=F(X)+G(x)+\\epsilon_2$$\n",
    " **Step 3.**\n",
    " $$\\epsilon_2=H(x)+\\epsilon_3$$\n",
    " Now:\n",
    " $$Y=F(x)+G(x)+H(x)+\\epsilon_3$$\n",
    " Finally, by adding weighting:\n",
    " $$Y=\\alpha F(x)+\\beta G(x)+\\gamma H(x)+\\epsilon_4$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
