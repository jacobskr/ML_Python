{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting (Hypothesis Boosting)\n",
    " - Combine several weak learners into a strong leaner.\n",
    " - Train predictors sequentially.\n",
    " \n",
    "## AdaBoost / Adaptive Boosting\n",
    "\n",
    "As above for Boosting:\n",
    " - Similar to human learning, the algorithm learns from past mistakes by focusing more on difficult problems it did not get right in prior learning.\n",
    " - It pays more attention to training instances that previously underfit.\n",
    " \n",
    " \n",
    " - Fit a sequence of weak learners (models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data.\n",
    " - The precistions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction.\n",
    " - The data modifications at each boosting iteration consist of applying weights $w_1, w_2, ... , w_n$ for each of the training samples.\n",
    " - Initially, those weights are all set to $w_i=1/N$, so that the first step simply trains a weak learner on the original data.\n",
    " - For each successive iteration, the sample weights are indiviually modified and the learning algorithm is reapplied to the reweighted data.\n",
    " - At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly.\n",
    " - As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "df.dropna(inplace=True)\n",
    "X = df[['pclass', 'sex', 'age']]\n",
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "X['sex'] = lb.fit_transform(X['sex'])\n",
    "y = df['survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(clf, X_train, y_train, X_test, y_test, train=True):\n",
    "    '''\n",
    "    print the accuracy score, classification report, and confusion matrix of classifier\n",
    "    '''\n",
    "    if train:\n",
    "        '''\n",
    "        Training Performance\n",
    "        '''\n",
    "        print(\"Train Result:\\n\")\n",
    "        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_train, clf.predict(X_train))))\n",
    "        print(\"Classification Report:\\n {} \\n\".format(classification_report(y_train, clf.predict(X_train))))\n",
    "        print(\"Confusion Matrix:  \\n {} \\n\".format(confusion_matrix(y_train, clf.predict(X_train))))\n",
    "        \n",
    "        res = cross_val_score(clf, X_train, y_train, cv=10, scoring='accuracy')\n",
    "        print(\"Average Accuracy: \\t {0:.4f}\".format(np.mean(res)))\n",
    "        print(\"Accuracy SD \\t\\t {0:.4f}\".format(np.std(res)))\n",
    "        \n",
    "    elif train==False:\n",
    "        '''\n",
    "        test performance\n",
    "        '''\n",
    "        print(\"Test Result:\\n\")\n",
    "        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_test, clf.predict(X_test))))\n",
    "        print(\"Classification Report:\\n {} \\n\".format(classification_report(y_test, clf.predict(X_test))))\n",
    "        print(\"Confusion Matrix:  \\n {} \\n\".format(confusion_matrix(y_test, clf.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_clf = AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "\n",
      "accuracy score: 0.8819\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.86      0.84        44\n",
      "           1       0.93      0.89      0.91        83\n",
      "\n",
      "   micro avg       0.88      0.88      0.88       127\n",
      "   macro avg       0.87      0.88      0.87       127\n",
      "weighted avg       0.88      0.88      0.88       127\n",
      " \n",
      "\n",
      "Confusion Matrix:  \n",
      " [[38  6]\n",
      " [ 9 74]] \n",
      "\n",
      "Average Accuracy: \t 0.7174\n",
      "Accuracy SD \t\t 0.1236\n"
     ]
    }
   ],
   "source": [
    "print_score(ada_clf, X_train, y_train, X_test, y_test, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "\n",
      "accuracy score: 0.7273\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.60      0.55        15\n",
      "           1       0.84      0.78      0.81        40\n",
      "\n",
      "   micro avg       0.73      0.73      0.73        55\n",
      "   macro avg       0.67      0.69      0.68        55\n",
      "weighted avg       0.75      0.73      0.73        55\n",
      " \n",
      "\n",
      "Confusion Matrix:  \n",
      " [[ 9  6]\n",
      " [ 9 31]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_score(ada_clf, X_train, y_train, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_clf = AdaBoostClassifier(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "\n",
      "accuracy score: 0.9528\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        44\n",
      "           1       0.96      0.96      0.96        83\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       127\n",
      "   macro avg       0.95      0.95      0.95       127\n",
      "weighted avg       0.95      0.95      0.95       127\n",
      " \n",
      "\n",
      "Confusion Matrix:  \n",
      " [[41  3]\n",
      " [ 3 80]] \n",
      "\n",
      "Average Accuracy: \t 0.7650\n",
      "Accuracy SD \t\t 0.1295\n"
     ]
    }
   ],
   "source": [
    "print_score(ada_clf, X_train, y_train, X_test, y_test, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "\n",
      "accuracy score: 0.7273\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.60      0.55        15\n",
      "           1       0.84      0.78      0.81        40\n",
      "\n",
      "   micro avg       0.73      0.73      0.73        55\n",
      "   macro avg       0.67      0.69      0.68        55\n",
      "weighted avg       0.75      0.73      0.73        55\n",
      " \n",
      "\n",
      "Confusion Matrix:  \n",
      " [[ 9  6]\n",
      " [ 9 31]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_score(ada_clf, X_train, y_train, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Try with grid search and increasing n_estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {'max_depth': [3, 4, 5, None],\n",
    "               'min_samples_split': range(2, 10),\n",
    "               'min_samples_leaf': range(2, 10),\n",
    "               'criterion': ['gini', 'entropy'],\n",
    "               'n_estimators': [500]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(rf_clf, params_grid,\n",
    "                          n_jobs=-1, cv=5,\n",
    "                          verbose=1, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 512 candidates, totalling 2560 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   22.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   60.0s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed: 10.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2560 out of 2560 | elapsed: 11.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'max_depth': [3, 4, 5, None], 'min_samples_split': range(2, 10), 'min_samples_leaf': range(2, 10), 'criterion': ['gini', 'entropy'], 'n_estimators': [500]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 2,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 500,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': 42,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(max_depth=None, min_samples_leaf=2, \n",
    "                                                    min_samples_split=2, criterion='gini',\n",
    "                                                    n_estimators=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_clf = AdaBoostClassifier(rf_clf, n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=2, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          learning_rate=1.0, n_estimators=100, random_state=None)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "\n",
      "accuracy score: 0.9449\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92        44\n",
      "           1       0.95      0.96      0.96        83\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       127\n",
      "   macro avg       0.94      0.94      0.94       127\n",
      "weighted avg       0.94      0.94      0.94       127\n",
      " \n",
      "\n",
      "Confusion Matrix:  \n",
      " [[40  4]\n",
      " [ 3 80]] \n",
      "\n",
      "Average Accuracy: \t 0.7900\n",
      "Accuracy SD \t\t 0.1213\n"
     ]
    }
   ],
   "source": [
    "print_score(ada_clf, X_train, y_train, X_test, y_test, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "\n",
      "accuracy score: 0.7273\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.60      0.55        15\n",
      "           1       0.84      0.78      0.81        40\n",
      "\n",
      "   micro avg       0.73      0.73      0.73        55\n",
      "   macro avg       0.67      0.69      0.68        55\n",
      "weighted avg       0.75      0.73      0.73        55\n",
      " \n",
      "\n",
      "Confusion Matrix:  \n",
      " [[ 9  6]\n",
      " [ 9 31]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_score(ada_clf, X_train, y_train, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting / Gradient Boosting Machine (GBM)\n",
    "Works for both regression and classification\n",
    "\n",
    " - Sequentially adding predictors\n",
    " - Each one correcting it's predecessor\n",
    " - Fit new predictor to the residual errors\n",
    " \n",
    " **Step 1.**\n",
    " $$Y=F(x_1)+\\epsilon$$\n",
    " **Step 2.**\n",
    " $$\\epsilon=G(x_2)+\\epsilon_2$$\n",
    " Subsituting (2) into (1), we get:\n",
    " $$Y=F(x_1)+G(x_2)+\\epsilon_2$$\n",
    " **Step 3.**\n",
    " $$\\epsilon_2=H(x_3)+\\epsilon_3$$\n",
    " Now:\n",
    " $$Y=F(x_1)+G(x_2)+H(x_3)+\\epsilon_3$$\n",
    " Finally, by adding weighting:\n",
    " $$Y=\\alpha F(x_1)+\\beta G(x_2)+\\gamma H(x_3)+\\epsilon_4$$\n",
    " \n",
    " \n",
    " Gradient Boosting Machine involves three elements:\n",
    "  - **Loss function to be optimized:** Loss function depends on the type of problem being solved. In the case of regression problems, mean squared error is used, and in classification problems, logarithmic loss will be used. In boostong, at least stage, unexplained loss from prior iterations will be optimized rather than starting from scratch.\n",
    "  - **Weak learner to make predictions:** Decision trees are used as a weak learner in gradient boosting.\n",
    "  - **Additive model to add weak learners to minimize loss function:** Trees are added one at a time and existing trees in the model are not changed. The gradient descent procedure is used to minimuze the loss when adding trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Base Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              n_iter_no_change=None, presort='auto', random_state=None,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc_clf = GradientBoostingClassifier()\n",
    "gbc_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "\n",
      "accuracy score: 0.9528\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        44\n",
      "           1       0.96      0.96      0.96        83\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       127\n",
      "   macro avg       0.95      0.95      0.95       127\n",
      "weighted avg       0.95      0.95      0.95       127\n",
      " \n",
      "\n",
      "Confusion Matrix:  \n",
      " [[41  3]\n",
      " [ 3 80]] \n",
      "\n",
      "Average Accuracy: \t 0.7918\n",
      "Accuracy SD \t\t 0.1144\n"
     ]
    }
   ],
   "source": [
    "print_score(gbc_clf, X_train, y_train, X_test, y_test, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "\n",
      "accuracy score: 0.7636\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.60      0.58        15\n",
      "           1       0.85      0.82      0.84        40\n",
      "\n",
      "   micro avg       0.76      0.76      0.76        55\n",
      "   macro avg       0.70      0.71      0.71        55\n",
      "weighted avg       0.77      0.76      0.77        55\n",
      " \n",
      "\n",
      "Confusion Matrix:  \n",
      " [[ 9  6]\n",
      " [ 7 33]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_score(gbc_clf, X_train, y_train, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost (Extreme Gradient Boosting)\n",
    "\n",
    "### Objective Function: Training Loss + Regularization\n",
    "$$Obj(\\Theta)=L(\\Theta)+\\Omega(\\Theta)$$\n",
    " - $L$ is the training loss function, and\n",
    " - $\\Omega$ is the regularization term.\n",
    " \n",
    "**Training Loss**\n",
    "\n",
    "The training loss measures how predictive our model is on training data.\n",
    "\n",
    "Example 1, Mean Squared Error for Linear RegressionL\n",
    "$$L(\\Theta)=\\sum_i(y_i-\\hat y_i)^2$$\n",
    "\n",
    "Example 2, Logistic Loss for Logistic Regression:\n",
    "$$L(\\Theta)=\\sum_i[y_i\\ln(1+e^{i\\hat y_i})+(1-y_i)\\ln(1+e^{i\\hat y_i})]$$\n",
    "\n",
    "\n",
    "**Regularization Term**\n",
    "The regularization term controls the complexity of the model, which helps us to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Titanic data again from start to finish**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Pre-Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['pclass', 'sex', 'age']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['sex'] = lb.fit_transform(X['sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(clf, X_train, y_train, X_test, y_test, train=True):\n",
    "    '''\n",
    "    print the accuracy score, classification report, and confusion matrix of classifier\n",
    "    '''\n",
    "    if train:\n",
    "        '''\n",
    "        Training Performance\n",
    "        '''\n",
    "        print(\"Train Result:\\n\")\n",
    "        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_train, clf.predict(X_train))))\n",
    "        print(\"Classification Report:\\n {} \\n\".format(classification_report(y_train, clf.predict(X_train))))\n",
    "        print(\"Confusion Matrix:  \\n {} \\n\".format(confusion_matrix(y_train, clf.predict(X_train))))\n",
    "        \n",
    "        res = cross_val_score(clf, X_train, y_train, cv=10, scoring='accuracy')\n",
    "        print(\"Average Accuracy: \\t {0:.4f}\".format(np.mean(res)))\n",
    "        print(\"Accuracy SD \\t\\t {0:.4f}\".format(np.std(res)))\n",
    "        \n",
    "    elif train==False:\n",
    "        '''\n",
    "        test performance\n",
    "        '''\n",
    "        print(\"Test Result:\\n\")\n",
    "        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_test, clf.predict(X_test))))\n",
    "        print(\"Classification Report:\\n {} \\n\".format(classification_report(y_test, clf.predict(X_test))))\n",
    "        print(\"Confusion Matrix:  \\n {} \\n\".format(confusion_matrix(y_test, clf.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost\n",
    "\n",
    "if !pip install xgboost doesn't work:\n",
    " - download xgboost whl file from [here](http://www.lfd.uci.edu/~gohlke/pythonlibs/) (make sure to match your python version and system architecture, e.g. \"xgboost-0.6-cp35-cp35m-win_amd64.whl\" for python 3.5 on 64-bit machine)\n",
    " - open command prompt\n",
    " - cd to your Downloads folder (or wherever you saved the whl file)\n",
    " - pip install xgboost-0.6-cp35-cp35m-win_amd64.whl (or whatever your whl file is named)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = xgb.XGBClassifier(max_depth=3, n_estimators=5000, learning_rate=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.2, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=5000,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "\n",
      "accuracy score: 0.9449\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93        47\n",
      "           1       0.96      0.95      0.96        80\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       127\n",
      "   macro avg       0.94      0.94      0.94       127\n",
      "weighted avg       0.95      0.94      0.94       127\n",
      " \n",
      "\n",
      "Confusion Matrix:  \n",
      " [[44  3]\n",
      " [ 4 76]] \n",
      "\n",
      "Average Accuracy: \t 0.7936\n",
      "Accuracy SD \t\t 0.0991\n"
     ]
    }
   ],
   "source": [
    "print_score(xgb_clf, X_train, y_train, X_test, y_test, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "\n",
      "accuracy score: 0.7455\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.92      0.61        12\n",
      "           1       0.97      0.70      0.81        43\n",
      "\n",
      "   micro avg       0.75      0.75      0.75        55\n",
      "   macro avg       0.71      0.81      0.71        55\n",
      "weighted avg       0.86      0.75      0.77        55\n",
      " \n",
      "\n",
      "Confusion Matrix:  \n",
      " [[11  1]\n",
      " [13 30]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_score(xgb_clf, X_train, y_train, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try with grid search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {'max_depth': range(1,10),\n",
    "               'learning_rate': [.001, .01, .1, .2, .3],\n",
    "               'n_estimators': [10000],\n",
    "               'n_jobs': [-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(xgb_clf, params_grid,\n",
    "                          n_jobs=-1, cv=5,\n",
    "                          verbose=1, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   51.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 225 out of 225 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'max_depth': range(1, 10), 'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3], 'n_estimators': [10000], 'n_jobs': [-1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_score': 0.5,\n",
       " 'booster': 'gbtree',\n",
       " 'colsample_bylevel': 1,\n",
       " 'colsample_bytree': 1,\n",
       " 'gamma': 0,\n",
       " 'learning_rate': 0.001,\n",
       " 'max_delta_step': 0,\n",
       " 'max_depth': 3,\n",
       " 'min_child_weight': 1,\n",
       " 'missing': None,\n",
       " 'n_estimators': 10000,\n",
       " 'n_jobs': -1,\n",
       " 'nthread': None,\n",
       " 'objective': 'binary:logistic',\n",
       " 'random_state': 0,\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 1,\n",
       " 'scale_pos_weight': 1,\n",
       " 'seed': None,\n",
       " 'silent': True,\n",
       " 'subsample': 1}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.001, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=10000,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf = xgb.XGBClassifier(max_depth=3, n_estimators=10000, learning_rate=0.001)\n",
    "xgb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "\n",
      "accuracy score: 0.9134\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.85      0.88        47\n",
      "           1       0.92      0.95      0.93        80\n",
      "\n",
      "   micro avg       0.91      0.91      0.91       127\n",
      "   macro avg       0.91      0.90      0.91       127\n",
      "weighted avg       0.91      0.91      0.91       127\n",
      " \n",
      "\n",
      "Confusion Matrix:  \n",
      " [[40  7]\n",
      " [ 4 76]] \n",
      "\n",
      "Average Accuracy: \t 0.7942\n",
      "Accuracy SD \t\t 0.0849\n"
     ]
    }
   ],
   "source": [
    "print_score(xgb_clf, X_train, y_train, X_test, y_test, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "\n",
      "accuracy score: 0.8182\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.92      0.69        12\n",
      "           1       0.97      0.79      0.87        43\n",
      "\n",
      "   micro avg       0.82      0.82      0.82        55\n",
      "   macro avg       0.76      0.85      0.78        55\n",
      "weighted avg       0.88      0.82      0.83        55\n",
      " \n",
      "\n",
      "Confusion Matrix:  \n",
      " [[11  1]\n",
      " [ 9 34]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_score(xgb_clf, X_train, y_train, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did a ton better on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
